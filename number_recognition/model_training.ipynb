{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac984dd3-9c51-464e-86b5-bc42cbbe3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import cv2\n",
    "#import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4d9186-4bd7-4c53-99a5-d9911e48493d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for folder in ['train','test','val']:\\n    for file in os.listdir(os.path.join('data/back_numbers_deinterlaced', folder, 'images')):\\n        \\n        filename = file.split('.')[0]+'.json'\\n        existing_filepath = os.path.join('data/back_numbers_deinterlaced','labels', filename)\\n        if os.path.exists(existing_filepath): \\n            new_filepath = os.path.join('data/back_numbers_deinterlaced',folder,'labels',filename)\\n            os.replace(existing_filepath, new_filepath) \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data/back_numbers_deinterlaced', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data/back_numbers_deinterlaced','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data/back_numbers_deinterlaced',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16b083e7-4fe4-42a3-bb48-2c371ac19267",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([\n",
    "                         alb.MotionBlur(blur_limit=41, p=0.5),\n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df20674-327c-4dd4-8063-0be24f4fc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data/back_numbers_deinterlaced', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data/back_numbers_deinterlaced', partition, 'images', image))\n",
    "\n",
    "        coords = []\n",
    "        labels = []\n",
    "        label_path = os.path.join('data/back_numbers_deinterlaced', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        height, width,_ = img.shape\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "                for i in range(len(label['shapes'])):\n",
    "                    co = [0,0,0.00001,0.00001]\n",
    "                    co[0] = label['shapes'][i]['points'][0][0]\n",
    "                    co[1] = label['shapes'][i]['points'][0][1]\n",
    "                    co[2] = label['shapes'][i]['points'][1][0]\n",
    "                    co[3] = label['shapes'][i]['points'][1][1]\n",
    "                    if co[0] > co[2]:\n",
    "                        c = co[0]\n",
    "                        co[0] = co[2]\n",
    "                        co[2] = c\n",
    "                    if co[1] > co[3]:\n",
    "                        c = co[1]\n",
    "                        co[1] = co[3]\n",
    "                        co[3] = c\n",
    "                    co = list(np.divide(co, [width,height,width,height]))\n",
    "                    coords.append(co)\n",
    "                    labels.append(label['shapes'][i]['label'])\n",
    "\n",
    "        else:\n",
    "            coords.append([0,0,0.00001,0.00001])\n",
    "            labels.append(\"background\")\n",
    "        try: \n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=coords, class_labels=labels)\n",
    "                cv2.imwrite(os.path.join('data/datasets', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bboxes'] = [[0,0,0,0]]\n",
    "                        annotation['class'] = [\"background\"] \n",
    "                    else: \n",
    "                        annotation['bboxes'] = augmented['bboxes']\n",
    "                        annotation['class'] = augmented['class_labels']\n",
    "                else: \n",
    "                    annotation['bboxes'] = [[0,0,0,0]]\n",
    "                    annotation['class'] = [\"background\"]\n",
    "\n",
    "                with open(os.path.join('data/datasets', partition, 'labels_old', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b34074d-f389-430c-838a-19c9b6b6ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_yolo(label_path):\n",
    "    with open(label_path) as f:\n",
    "        label = json.load(f)\n",
    "    return label['class'], label['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "320cf545-3604-4396-acf9-b78170e68534",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for file in os.listdir(os.path.join('data/datasets/', partition, 'labels_old')):\n",
    "        path = os.path.join('data/datasets', partition, 'labels_old',file)\n",
    "        labels , boxes = convert_labels_yolo(path)\n",
    "        yolo = \"\"\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] ==\"background\":\n",
    "                break\n",
    "            yolo = yolo + labels[i]+ ' '\n",
    "            average_x = (boxes[i][2]+ boxes[i][0])/2\n",
    "            average_y = (boxes[i][3]+ boxes[i][1])/2\n",
    "            width = boxes[i][2]- boxes[i][0]\n",
    "            height = boxes[i][3]- boxes[i][1]\n",
    "            if i!= len(labels)-1:\n",
    "                yolo+= \"{} {} {} {}\\n\".format(average_x, average_y, width, height)\n",
    "            else:\n",
    "                yolo+= \"{} {} {} {}\".format(average_x, average_y, width, height)\n",
    "        file_path = os.path.join('data' ,'datasets', partition, 'labels',file.split(\".\")[0]+ \".\"+ file.split(\".\")[1] + \".txt\")\n",
    "        with open(file_path, 'w') as file:\n",
    "    \n",
    "            # Write the string to the file\n",
    "            file.write(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7e6fb-3564-4bf1-9a25-153b8746ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.205 🚀 Python-3.11.4 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/qlaventu/.config/Ultralytics/Arial.ttf'...\n",
      "100%|██████████| 755k/755k [00:00<00:00, 55.7MB/s]\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/train/labels... 27900 images, 2340 backgrounds, 0 corrupt: 100%|██████████| 27900/27900 [00:20<00:00, 1349.46it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /project_ghent/mt23/number_recognition/datasets/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/val/labels... 6000 images, 360 backgrounds, 0 corrupt: 100%|██████████| 6000/6000 [00:05<00:00, 1007.35it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /project_ghent/mt23/number_recognition/datasets/val/labels.cache\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      2.36G      1.393      2.095      1.506         26        640: 100%|██████████| 1744/1744 [07:12<00:00,  4.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:47<00:00,  3.96it/s]\n",
      "                   all       6000      11700      0.799      0.542      0.634       0.43\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      2.37G      1.257      1.278      1.363         33        640:  26%|██▌       | 452/1744 [01:50<05:18,  4.05it/s]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config.yaml\", epochs=10)  # train the model\n",
    "metrics = model.val()  # evaluate model performance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431cd0f-2170-46d7-9546-b5051b07d3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951277f3-d163-43db-b31a-cd2c9d7a208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease               \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists... Done\u001b[33m\u001b[33m\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "62 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 62 not upgraded.\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.24.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.0.205-py3-none-any.whl (644 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.2 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (1.24.4)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (9.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (1.11.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (2.0.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2023.5.7)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Installing collected packages: thop, ultralytics\n",
      "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.205\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y libgl1-mesa-glx\n",
    "!pip install opencv-python\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adfea6ea-25b1-45b6-b2ae-5ed1c9495b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x640 1 0, 1 2, 1 4, 12.2ms\n",
      "Speed: 3.6ms preprocess, 12.2ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed50db2ad0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MODEL_PATH = os.path.join('runs','detect','train4','weights','best.pt')\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "img = cv.imread(\"data/back_numbers_deinterlaced/test/images/f4b87855-7248-11ee-901a-fcb3bc5cd112.jpg\")\n",
    "results = model(img)\n",
    "boxes = results[0].boxes.xyxy\n",
    "for i in boxes:\n",
    "    top = tuple([int(i[0].item()),int(i[1].item())])\n",
    "    bottom = tuple([int(i[2].item()),int(i[3].item())])\n",
    "    cv.rectangle(img,top,bottom, (255,0,0), 2)\n",
    "\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb35752-a9a4-4417-ba66-d2a064905edb",
   "metadata": {},
   "source": [
    "# train model with 5000 train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5560a4f-df3e-4c02-9543-ff4a6128c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config_5000.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train6\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/qlaventu/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 755k/755k [00:00<00:00, 54.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train6', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "WARNING ⚠️ NMS time limit 0.550s exceeded\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/train_5000/labels... 0 images, 5000 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:01<00:00, 2937.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ No labels found in /project_ghent/mt23/number_recognition/datasets/train_5000/labels.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /project_ghent/mt23/number_recognition/datasets/train_5000/labels.cache\n",
      "WARNING ⚠️ No labels found in /project_ghent/mt23/number_recognition/datasets/train_5000/labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/val/labels.cache... 6000 images, 360 backgrounds, 0 corrupt: 100%|██████████| 6000/6000 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train6/labels.jpg... \n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train6\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.43G          0      103.2          0          0        640: 100%|██████████| 313/313 [01:17<00:00,  4.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:51<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      2.32G          0      59.64          0          0        640: 100%|██████████| 313/313 [01:07<00:00,  4.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.44G          0      22.97          0          0        640: 100%|██████████| 313/313 [01:07<00:00,  4.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.44G          0      6.751          0          0        640: 100%|██████████| 313/313 [01:06<00:00,  4.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      2.32G          0      1.745          0          0        640: 100%|██████████| 313/313 [01:06<00:00,  4.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      2.31G          0     0.4102          0          0        640: 100%|██████████| 313/313 [01:11<00:00,  4.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      2.32G          0     0.1073          0          0        640: 100%|██████████| 313/313 [01:08<00:00,  4.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      2.32G          0    0.03092          0          0        640: 100%|██████████| 313/313 [01:08<00:00,  4.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:30<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      2.32G          0    0.02444          0          0        640: 100%|██████████| 313/313 [01:09<00:00,  4.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      2.31G          0     0.0223          0          0        640: 100%|██████████| 313/313 [01:08<00:00,  4.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:29<00:00,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.288 hours.\n",
      "Optimizer stripped from runs/detect/train6/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train6/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train6/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "Model summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:57<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00203     0.0303    0.00109   0.000204\n",
      "                     0       6000        360          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     1       6000       3960   0.000259   0.000758    0.00013   2.59e-05\n",
      "                     2       6000        840          0          0          0          0\n",
      "                     3       6000       1620      0.019     0.0475       0.01    0.00189\n",
      "                     4       6000       1440          0          0          0          0\n",
      "                     5       6000        660          0          0          0          0\n",
      "                     6       6000       1500          0          0          0          0\n",
      "                     7       6000        600      0.001      0.255   0.000724   0.000123\n",
      "                     8       6000        480          0          0          0          0\n",
      "                     9       6000        240          0          0          0          0\n",
      "Speed: 0.4ms preprocess, 3.6ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config_5000.yaml\", epochs=10)  # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa051e83-4102-4bb8-b0eb-57b85a98e90a",
   "metadata": {},
   "source": [
    "# train model with 10000 train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d37b48-28b4-4ca2-9549-c32005dcdcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config_10000.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train7\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train7', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/train_10000/labels... 0 images, 10000 backgrounds, 0 corrupt: 100%|██████████| 10000/10000 [00:03<00:00, 2754.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ No labels found in /project_ghent/mt23/number_recognition/datasets/train_10000/labels.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /project_ghent/mt23/number_recognition/datasets/train_10000/labels.cache\n",
      "WARNING ⚠️ No labels found in /project_ghent/mt23/number_recognition/datasets/train_10000/labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/val/labels.cache... 6000 images, 360 backgrounds, 0 corrupt: 100%|██████████| 6000/6000 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train7/labels.jpg... \n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train7\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.45G          0      91.43          0          0        640: 100%|██████████| 625/625 [02:29<00:00,  4.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:02<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      2.32G          0      24.81          0          0        640: 100%|██████████| 625/625 [02:44<00:00,  3.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:51<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.32G          0      1.971          0          0        640: 100%|██████████| 625/625 [02:50<00:00,  3.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:54<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.32G          0    0.06711          0          0        640: 100%|██████████| 625/625 [03:13<00:00,  3.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:02<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      2.32G          0    0.01007          0          0        640: 100%|██████████| 625/625 [03:14<00:00,  3.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:06<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      2.32G          0   0.003523          0          0        640: 100%|██████████| 625/625 [03:24<00:00,  3.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:19<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      2.32G          0   0.002126          0          0        640: 100%|██████████| 625/625 [03:08<00:00,  3.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:58<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      2.32G          0  0.0004378          0          0        640: 100%|██████████| 625/625 [03:15<00:00,  3.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:59<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      2.32G          0  3.702e-05          0          0        640: 100%|██████████| 625/625 [03:28<00:00,  3.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:02<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      2.32G          0  7.769e-07          0          0        640: 100%|██████████| 625/625 [03:31<00:00,  2.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:01<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700   5.06e-05   0.000455   2.54e-05   2.54e-06\n",
      "\n",
      "10 epochs completed in 0.725 hours.\n",
      "Optimizer stripped from runs/detect/train7/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train7/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train7/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "Model summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  19%|█▊        | 35/188 [00:28<11:59,  4.70s/it]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config_10000.yaml\", epochs=10)  # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de7b4a-c609-4992-8ce0-a381c3091d28",
   "metadata": {},
   "source": [
    "# train model with 15000 train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336d3d36-18e0-4222-8b4d-7c87894366e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config_15000.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train8\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train8', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/train_15000/labels... 2100 images, 13045 backgrounds, 0 corrupt: 100%|██████████| 15000/15000 [00:06<00:00, 2179.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /project_ghent/mt23/number_recognition/datasets/train_15000/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/val/labels.cache... 6000 images, 360 backgrounds, 0 corrupt: 100%|██████████| 6000/6000 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train8\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.32G      1.454      19.57      1.567          0        640: 100%|██████████| 938/938 [04:42<00:00,  3.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:18<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700    0.00899      0.708     0.0861     0.0456\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      2.35G      1.444      9.552      1.553          4        640: 100%|██████████| 938/938 [04:11<00:00,  3.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:51<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.075      0.592       0.21      0.109\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.33G      1.466      5.038      1.506          0        640: 100%|██████████| 938/938 [03:57<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:46<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.148      0.352      0.269      0.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.36G      1.451      3.963      1.468          3        640: 100%|██████████| 938/938 [03:55<00:00,  3.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:48<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.203      0.583      0.367      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      2.37G      1.399      3.646      1.411          2        640: 100%|██████████| 938/938 [03:55<00:00,  3.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:48<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.289      0.609      0.471       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      2.36G      1.389       3.45      1.409          3        640: 100%|██████████| 938/938 [03:55<00:00,  3.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:48<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.286      0.682       0.54       0.36\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      2.36G      1.356      3.303      1.383          0        640: 100%|██████████| 938/938 [03:55<00:00,  3.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:48<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.284      0.709      0.555      0.359\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      2.36G      1.343      3.243      1.369          4        640: 100%|██████████| 938/938 [03:52<00:00,  4.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:50<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.174      0.769      0.586      0.391\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      2.36G      1.278      3.147      1.323          0        640: 100%|██████████| 938/938 [03:53<00:00,  4.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:49<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.339      0.685      0.565      0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      2.36G       1.24      3.098      1.304          2        640: 100%|██████████| 938/938 [03:54<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:50<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.266      0.731      0.628      0.441\n",
      "\n",
      "10 epochs completed in 0.826 hours.\n",
      "Optimizer stripped from runs/detect/train8/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train8/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train8/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "Model summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.266      0.731      0.629      0.441\n",
      "                     0       6000        360      0.198      0.758      0.609      0.471\n",
      "                     1       6000       3960      0.246      0.799      0.712      0.461\n",
      "                     2       6000        840      0.224      0.718      0.673      0.452\n",
      "                     3       6000       1620       0.29      0.764      0.696      0.468\n",
      "                     4       6000       1440      0.419      0.802      0.785      0.519\n",
      "                     5       6000        660      0.159      0.629      0.492      0.325\n",
      "                     6       6000       1500      0.273      0.692      0.584      0.406\n",
      "                     7       6000        600      0.188       0.78      0.657      0.456\n",
      "                     8       6000        480      0.145      0.685      0.337      0.247\n",
      "                     9       6000        240      0.519      0.679      0.743      0.607\n",
      "Speed: 0.3ms preprocess, 2.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config_15000.yaml\", epochs=10)  # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24881cbd-be4b-4ebd-9498-1d298c0258c4",
   "metadata": {},
   "source": [
    "# train model with 20000 train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3059908-8c68-451e-8761-4243ae7e5222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config_20000.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train9', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/train_20000/labels... 12100 images, 8914 backgrounds, 0 corrupt: 100%|██████████| 20000/20000 [00:13<00:00, 1524.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /project_ghent/mt23/number_recognition/datasets/train_20000/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/val/labels.cache... 6000 images, 360 backgrounds, 0 corrupt: 100%|██████████| 6000/6000 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train9/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train9\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.37G      1.485      3.106      1.583         15        640: 100%|██████████| 1250/1250 [06:46<00:00,  3.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.648      0.431      0.525      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      2.37G      1.377      1.946      1.449         26        640: 100%|██████████| 1250/1250 [08:25<00:00,  2.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:15<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.782      0.531       0.61      0.412\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.34G      1.328      1.717      1.409         21        640: 100%|██████████| 1250/1250 [08:56<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:17<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.829      0.558      0.648      0.449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.35G       1.27      1.587      1.357         21        640: 100%|██████████| 1250/1250 [10:19<00:00,  2.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:09<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700       0.83       0.62      0.705      0.486\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      2.35G      1.223      1.505      1.329         15        640: 100%|██████████| 1250/1250 [09:22<00:00,  2.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:11<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.852      0.595      0.697      0.477\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      2.35G      1.163      1.448      1.289         22        640: 100%|██████████| 1250/1250 [10:19<00:00,  2.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:11<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.823      0.573      0.694      0.474\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      2.34G      1.105      1.371      1.257         14        640: 100%|██████████| 1250/1250 [08:49<00:00,  2.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:08<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.864      0.615      0.722      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      2.34G      1.056      1.329      1.233         18        640: 100%|██████████| 1250/1250 [08:12<00:00,  2.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:19<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.848      0.632      0.733      0.499\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      2.34G      1.006      1.276        1.2         12        640: 100%|██████████| 1250/1250 [09:23<00:00,  2.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:17<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.864      0.665      0.753      0.513\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      2.34G     0.9576      1.243      1.181         22        640: 100%|██████████| 1250/1250 [10:08<00:00,  2.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:07<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.868      0.663      0.759      0.519\n",
      "\n",
      "10 epochs completed in 1.738 hours.\n",
      "Optimizer stripped from runs/detect/train9/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train9/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train9/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "Model summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:05<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.868      0.661      0.759      0.519\n",
      "                     0       6000        360      0.973      0.606       0.75      0.582\n",
      "                     1       6000       3960      0.831      0.655      0.759      0.516\n",
      "                     2       6000        840      0.935      0.688      0.784        0.5\n",
      "                     3       6000       1620      0.853      0.713      0.787      0.554\n",
      "                     4       6000       1440      0.895      0.771       0.86      0.593\n",
      "                     5       6000        660      0.808      0.572       0.62      0.366\n",
      "                     6       6000       1500      0.839       0.69      0.778      0.519\n",
      "                     7       6000        600      0.917      0.516      0.709      0.459\n",
      "                     8       6000        480      0.839      0.571      0.644      0.454\n",
      "                     9       6000        240      0.788      0.829      0.897      0.648\n",
      "Speed: 0.6ms preprocess, 2.8ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config_20000.yaml\", epochs=10)  # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43fb33-04fc-4453-a4d1-80db30ce943b",
   "metadata": {},
   "source": [
    "# train model with 25000 train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c892b6-8437-4907-a904-94be7b60e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config_25000.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train11\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train11', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/train_25000/labels.cache... 22100 images, 4749 backgrounds, 0 corrupt: 100%|██████████| 25000/25000 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/val/labels.cache... 6000 images, 360 backgrounds, 0 corrupt: 100%|██████████| 6000/6000 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train11/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train11\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.32G      1.419      2.362      1.537         12        640: 100%|██████████| 1563/1563 [06:41<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:54<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.805       0.56      0.636       0.42\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      2.36G       1.27      1.404      1.373         17        640: 100%|██████████| 1563/1563 [06:38<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:50<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.809       0.58      0.661       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.35G      1.212      1.228       1.33         18        640: 100%|██████████| 1563/1563 [06:30<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:51<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.837      0.616      0.707      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.37G      1.145      1.118      1.283         13        640: 100%|██████████| 1563/1563 [06:31<00:00,  3.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:51<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.857      0.607      0.711      0.484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      2.38G      1.095      1.054      1.252          7        640: 100%|██████████| 1563/1563 [06:29<00:00,  4.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.846      0.658      0.754      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      2.38G      1.027      0.986      1.211         11        640: 100%|██████████| 1563/1563 [06:34<00:00,  3.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:59<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700       0.87      0.656      0.749       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      2.38G     0.9735     0.9356      1.182         11        640: 100%|██████████| 1563/1563 [06:41<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:08<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700       0.87      0.618      0.737      0.506\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      2.37G     0.9211     0.8961      1.156         14        640: 100%|██████████| 1563/1563 [06:50<00:00,  3.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:53<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.849      0.661      0.753      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      2.38G     0.8747     0.8617      1.131         16        640: 100%|██████████| 1563/1563 [06:44<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:04<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.868      0.646      0.759      0.518\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      2.38G     0.8295     0.8248      1.113         16        640: 100%|██████████| 1563/1563 [06:53<00:00,  3.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [01:08<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.866      0.687      0.785      0.541\n",
      "\n",
      "10 epochs completed in 1.279 hours.\n",
      "Optimizer stripped from runs/detect/train11/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train11/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train11/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "Model summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:51<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.867      0.687      0.785      0.542\n",
      "                     0       6000        360      0.953      0.689       0.84      0.634\n",
      "                     1       6000       3960      0.818       0.64      0.758      0.489\n",
      "                     2       6000        840      0.921      0.693      0.798      0.512\n",
      "                     3       6000       1620      0.886      0.712      0.814      0.591\n",
      "                     4       6000       1440      0.859       0.81      0.867      0.602\n",
      "                     5       6000        660      0.645      0.602      0.605      0.387\n",
      "                     6       6000       1500      0.869      0.666      0.777      0.519\n",
      "                     7       6000        600      0.904       0.62      0.756      0.484\n",
      "                     8       6000        480      0.913      0.568       0.69      0.483\n",
      "                     9       6000        240      0.901      0.867      0.944      0.716\n",
      "Speed: 0.3ms preprocess, 2.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config_25000.yaml\", epochs=10)  # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0c844-4b69-4cff-9964-18ceca1fe213",
   "metadata": {},
   "source": [
    "\n",
    "remarks: need to use agnostic_nms in training to surpress different classes on the same place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5640d-7341-4fd0-92f7-6198afca368c",
   "metadata": {},
   "source": [
    "# train model with image shape 450, 450 and add black borders\n",
    "\n",
    "model uses letterbox function: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/predictor.py#L139\n",
    "Letterbox is hier beschreven: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/augment.py\n",
    "vergroot of verkleint afbeelding to de grootste zijde de gewenste afmetingen heeft en verkleint afbeelding evenredig.\n",
    "-> aspect ratio blijft behouden en dus geen vervorming.\n",
    "voegt dan border toe met kleur 114, 114, 114 -> grijs\n",
    "beide volgende opties lijken geen goede keuze:\n",
    "optie 1: elk nummer zal een andere grootte hebben -> moeilijker\n",
    "optie 2: is exact zelfde als zou gebeuren door yolo zelf maar met zwarte rande ipv grijs\n",
    "wel optie om de input size wat te verkleinen zodat minder uitvergroot wordt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "285ea4c6-e026-4cce-bb7c-51508f8eb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for file in os.listdir(os.path.join('datasets','train', 'images')):\n",
    "    label_path = os.path.join('datasets','train','labels', file.split('.')[0] + '.' + file.split('.')[1] + '.txt')\n",
    "    img = cv2.imread(os.path.join('datasets','train','images', file))\n",
    "    new_image = np.zeros((450,450,3), dtype = int)\n",
    "    h,w, _ = img.shape\n",
    "    added_h = 0\n",
    "    added_w = 0\n",
    "    if h <450:\n",
    "        added_h = 450-h\n",
    "        if w<450:\n",
    "            added_w = 450-w\n",
    "        else:\n",
    "            new_image = np.zeros((450,w,3),  dtype = int)\n",
    "    else:\n",
    "        if w<450:\n",
    "            added_w = 450-w\n",
    "            new_image = np.zeros((h,450,3),  dtype = int)\n",
    "        else:\n",
    "            new_image = np.zeros((h,w,3), dtype = int)\n",
    "    new_image[added_h//2: added_h//2+h, added_w//2: added_w//2+w] = img\n",
    "    #cv2.imwrite(os.path.join('datasets','train_450,450', 'images', file), new_image)\n",
    "    with open(label_path) as f:\n",
    "        with open(os.path.join('datasets','train_450,450','labels', file.split('.')[0] + '.' + file.split('.')[1] + '.txt'), 'w') as f_n:\n",
    "            for line in f.readlines():\n",
    "                c, tw, th, bw, bh = line.strip().split(' ')\n",
    "                text = c+' '+str((float(tw)*w+added_w//2)/new_image.shape[1])+' '+str((float(th)*h+added_h//2)/new_image.shape[0])+' '+str((float(bw)*w)/new_image.shape[1])+' '+str((float(bh)*h)/new_image.shape[0])\n",
    "                f_n.write(text)\n",
    "                if line[-1:] == '\\n':\n",
    "                    f_n.write('\\n')\n",
    "            f_n.close()\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa9910-4e35-4bb4-b213-c1e34f081e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config_450,450.yaml\", epochs=10, imgsz = 450)  # train the model\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f609f14f-daed-4bf6-9a93-efa408aa72c7",
   "metadata": {},
   "source": [
    "## try model with making image square by adding black borders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30f12d-967e-4651-9319-22490fb268ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for file in os.listdir(os.path.join('datasets','train', 'images')):\n",
    "    label_path = os.path.join('datasets','train','labels', file.split('.')[0] + '.' + file.split('.')[1] + '.txt')\n",
    "    img = cv2.imread(os.path.join('datasets','train','images', file))\n",
    "    h,w, _ = img.shape\n",
    "    shape = max(h,w)\n",
    "    added_h = shape-h\n",
    "    added_w = shape-w\n",
    "    new_image = np.zeros((shape,shape,3), dtype = int)\n",
    "    if added_h >0:\n",
    "        new_image[added_h//2:added_h//2+h] = img\n",
    "\n",
    "    else:\n",
    "        if added_w > 0:\n",
    "            new_image[:,added_w//2:added_w//2+w] = img\n",
    "        else:\n",
    "            new_image = img\n",
    "    #cv2.imwrite(os.path.join('datasets','train_square', 'images', file), new_image)\n",
    "    with open(label_path) as f:\n",
    "        with open(os.path.join('datasets','train_square','labels', file.split('.')[0] + '.' + file.split('.')[1] + '.txt'), 'w') as f_n:\n",
    "            for line in f.readlines():\n",
    "                c, tw, th, bw, bh = line.strip().split(' ')\n",
    "                text = c+' '+str((float(tw)*w+added_w//2)/new_image.shape[1])+' '+str((float(th)*h+added_h//2)/new_image.shape[0])+' '+str((float(bw)*w)/new_image.shape[1])+' '+str((float(bh)*h)/new_image.shape[0])\n",
    "                f_n.write(text)\n",
    "                if line[-1:] == '\\n':\n",
    "                    f_n.write('\\n')\n",
    "            f_n.close()\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4645e8-c6bd-4ca9-8835-ce594e003382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config_square.yaml\", epochs=10, imgsz = 450)  # train the model\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d84881-8478-460d-a758-0d8255d970ad",
   "metadata": {},
   "source": [
    "## train model with image size = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235217b4-6b53-41a6-82e2-7a667d9ee9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config.yaml, epochs=10, patience=50, batch=16, imgsz=450, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train12, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train12\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train12', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "WARNING ⚠️ imgsz=[450] must be multiple of max stride 32, updating to [480]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/train/labels.cache... 27900 images, 2340 backgrounds, 0 corrupt: 100%|██████████| 27900/27900 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project_ghent/mt23/number_recognition/datasets/val/labels.cache... 6000 images, 360 backgrounds, 0 corrupt: 100%|██████████| 6000/6000 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train12/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 480 train, 480 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train12\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      1.26G      1.394      1.987      1.419         26        480: 100%|██████████| 1744/1744 [05:18<00:00,  5.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:49<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.814       0.55      0.645      0.435\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      1.29G      1.232      1.202      1.259         25        480: 100%|██████████| 1744/1744 [05:06<00:00,  5.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:42<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.816      0.578      0.651      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      1.26G      1.165      1.041      1.208         22        480: 100%|██████████| 1744/1744 [05:01<00:00,  5.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:44<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.829      0.622      0.705       0.49\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      1.39G       1.09     0.9418      1.163         20        480: 100%|██████████| 1744/1744 [05:02<00:00,  5.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:46<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.833      0.637      0.735      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      1.27G      1.041     0.8756      1.135         23        480: 100%|██████████| 1744/1744 [05:02<00:00,  5.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:44<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.836      0.672      0.755      0.516\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      1.27G     0.9841     0.8138      1.107         30        480: 100%|██████████| 1744/1744 [05:04<00:00,  5.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:46<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.846      0.666      0.759      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      1.26G     0.9255     0.7605       1.08         24        480: 100%|██████████| 1744/1744 [05:05<00:00,  5.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:47<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.855      0.671      0.762      0.522\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      1.38G     0.8812     0.7219      1.061         23        480: 100%|██████████| 1744/1744 [05:03<00:00,  5.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:47<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.849      0.684       0.77      0.535\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      1.26G     0.8337     0.6807      1.039         24        480: 100%|██████████| 1744/1744 [05:08<00:00,  5.65it/s] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:45<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.866      0.699       0.78      0.535\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      1.38G     0.7933     0.6422      1.023         25        480: 100%|██████████| 1744/1744 [05:05<00:00,  5.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:45<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.858        0.7      0.782      0.545\n",
      "\n",
      "10 epochs completed in 0.994 hours.\n",
      "Optimizer stripped from runs/detect/train12/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train12/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train12/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "Model summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6000      11700      0.857        0.7      0.782      0.545\n",
      "                     0       6000        360      0.832      0.731      0.831      0.599\n",
      "                     1       6000       3960      0.863      0.635       0.77      0.503\n",
      "                     2       6000        840      0.872      0.663      0.793      0.526\n",
      "                     3       6000       1620      0.914      0.773      0.841      0.592\n",
      "                     4       6000       1440      0.919      0.802      0.885      0.622\n",
      "                     5       6000        660      0.758      0.555      0.614      0.414\n",
      "                     6       6000       1500       0.84      0.773      0.836      0.587\n",
      "                     7       6000        600      0.753      0.621      0.675      0.439\n",
      "                     8       6000        480       0.86      0.554      0.628      0.447\n",
      "                     9       6000        240       0.96      0.889      0.948      0.725\n",
      "Speed: 0.2ms preprocess, 1.5ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config.yaml\", epochs=10, imgsz = 450)  # train the model\n",
    "#run 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc687cf-2e81-4042-a6f7-37da59d16db6",
   "metadata": {},
   "source": [
    "## train model with agnosic_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b0a1d6-c7d1-4551-8e33-a5807b0bc4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.219 🚀 Python-3.11.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train13, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=True, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train13\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa843fd8400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/connection.py\", line 947, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train13', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43magnostic_nms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# train the model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/model.py:338\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:190\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:286\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(world_size)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:251\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Dataloaders\u001b[39;00m\n\u001b[1;32m    250\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(world_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/models/yolo/detect/train.py:46\u001b[0m, in \u001b[0;36mDetectionTrainer.get_dataloader\u001b[0;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(rank):  \u001b[38;5;66;03m# init dataset *.cache only once if DDP\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m shuffle \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m shuffle:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/models/yolo/detect/train.py:40\u001b[0m, in \u001b[0;36mDetectionTrainer.build_dataset\u001b[0;34m(self, img_path, mode, batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mBuild YOLO Dataset.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(de_parallel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_yolo_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/data/build.py:80\u001b[0m, in \u001b[0;36mbuild_yolo_dataset\u001b[0;34m(cfg, img_path, batch, data, mode, rect, stride)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_yolo_dataset\u001b[39m(cfg, img_path, batch, data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, rect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build YOLO Dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mYOLODataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# augmentation\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: probably add a get_hyps_from_cfg function\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# rectangular batches\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_cls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_segments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msegment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_keypoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/data/dataset.py:41\u001b[0m, in \u001b[0;36mYOLODataset.__init__\u001b[0;34m(self, data, use_segments, use_keypoints, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_segments \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_keypoints), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan not use both segments and keypoints.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/data/base.py:72\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;241m=\u001b[39m prefix\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfraction \u001b[38;5;241m=\u001b[39m fraction\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_img_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_labels()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_labels(include_class\u001b[38;5;241m=\u001b[39mclasses)  \u001b[38;5;66;03m# single_cls and include_class\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ultralytics/data/base.py:106\u001b[0m, in \u001b[0;36mBaseDataset.get_img_files\u001b[0;34m(self, img_path)\u001b[0m\n\u001b[1;32m    104\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(p)  \u001b[38;5;66;03m# os-agnostic\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_dir():  \u001b[38;5;66;03m# dir\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     f \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m**\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*.*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# F = list(p.rglob('*.*'))  # pathlib\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_file():  \u001b[38;5;66;03m# file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/glob.py:28\u001b[0m, in \u001b[0;36mglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, include_hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mglob\u001b[39m(pathname, \u001b[38;5;241m*\u001b[39m, root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dir_fd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m         include_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of paths matching a pathname pattern.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    The pattern may contain simple shell-style wildcards a la\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    zero or more directories and subdirectories.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(iglob(pathname, root_dir\u001b[38;5;241m=\u001b[39mroot_dir, dir_fd\u001b[38;5;241m=\u001b[39mdir_fd, recursive\u001b[38;5;241m=\u001b[39mrecursive,\n\u001b[1;32m     29\u001b[0m                       include_hidden\u001b[38;5;241m=\u001b[39minclude_hidden))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/glob.py:96\u001b[0m, in \u001b[0;36m_iglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, dironly, include_hidden)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     glob_in_dir \u001b[38;5;241m=\u001b[39m _glob0\n\u001b[0;32m---> 96\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdirs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minclude_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_hidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/glob.py:97\u001b[0m, in \u001b[0;36m_iglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, dironly, include_hidden)\u001b[0m\n\u001b[1;32m     95\u001b[0m     glob_in_dir \u001b[38;5;241m=\u001b[39m _glob0\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirname \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m---> 97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minclude_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_hidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/glob.py:136\u001b[0m, in \u001b[0;36m_glob2\u001b[0;34m(dirname, pattern, dir_fd, dironly, include_hidden)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _isrecursive(pattern)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m pattern[:\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _rlistdir(dirname, dir_fd, dironly,\n\u001b[1;32m    137\u001b[0m                      include_hidden\u001b[38;5;241m=\u001b[39minclude_hidden)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/glob.py:181\u001b[0m, in \u001b[0;36m_rlistdir\u001b[0;34m(dirname, dir_fd, dironly, include_hidden)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rlistdir\u001b[39m(dirname, dir_fd, dironly, include_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 181\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[43m_listdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m include_hidden \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(x):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/glob.py:177\u001b[0m, in \u001b[0;36m_listdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_listdir\u001b[39m(dirname, dir_fd, dironly):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(_iterdir(dirname, dir_fd, dironly)) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(it)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/glob.py:162\u001b[0m, in \u001b[0;36m_iterdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dironly \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m fsencode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m fsencode(entry\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "res = model.train(data=\"config.yaml\", epochs=10,agnostic_nms = True)  # train the model\n",
    "#run 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43a536-a06f-482b-8595-87942e1eb795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
