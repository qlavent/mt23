{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "18ce0100-e535-4872-bdf0-66f3b0c38434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import sys\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "92f9eaad-639d-4d28-8391-391af61613fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1080 Ti\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (11): Concat()\n",
       "      (12): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (14): Concat()\n",
       "      (15): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Concat()\n",
       "      (18): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Concat()\n",
       "      (21): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"runs/detect/train4/weights/best.pt\"\n",
    "recognition_model = YOLO(model_path)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.device(\"cuda\"))\n",
    "device = torch.device(\"cuda\")\n",
    "recognition_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6e4b4f91-3b17-4cb9-a204-015f9b3d2f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.14285714285663267\n"
     ]
    }
   ],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    - box1: Tuple or List representing (x1, y1, x2, y2) of the first box.\n",
    "    - box2: Tuple or List representing (x1, y1, x2, y2) of the second box.\n",
    "\n",
    "    Returns:\n",
    "    - IoU: Intersection over Union.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract coordinates\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    x_intersection = max(0, min(x2_1, x2_2) - max(x1_1, x1_2))\n",
    "    y_intersection = max(0, min(y2_1, y2_2) - max(y1_1, y1_2))\n",
    "\n",
    "    # Calculate areas of boxes and intersection\n",
    "    area_box1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area_box2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    area_intersection = x_intersection * y_intersection\n",
    "\n",
    "    # Calculate Union\n",
    "    area_union = area_box1 + area_box2 - area_intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = area_intersection / (area_union + 1e-10)  # Add a small epsilon to avoid division by zero\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Example usage:\n",
    "box1 = (0, 0, 4, 4)\n",
    "box2 = (2, 2, 6, 6)\n",
    "\n",
    "iou = calculate_iou(box1, box2)\n",
    "print(f\"IoU: {iou}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc4580-af79-4b74-be33-6e14ecf77022",
   "metadata": {},
   "source": [
    "## testing whitout preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aac3c6b4-a6b8-4fc5-a398-a3da5afebcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 18.7ms\n",
      "Speed: 0.1ms preprocess, 18.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.4ms\n",
      "Speed: 0.1ms preprocess, 15.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.6ms\n",
      "Speed: 0.1ms preprocess, 15.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.2ms\n",
      "Speed: 0.1ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 16.5ms\n",
      "Speed: 0.1ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.1ms\n",
      "Speed: 0.0ms preprocess, 15.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.5ms\n",
      "Speed: 0.0ms preprocess, 15.5ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.9ms\n",
      "Speed: 0.0ms preprocess, 15.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.9ms\n",
      "Speed: 0.0ms preprocess, 15.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.469890594482422. Dividing input by 255.\n",
      "0: 224x224 (no detections), 15.6ms\n",
      "Speed: 0.1ms preprocess, 15.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "overal_digits = 0\n",
    "overal_predicted_digits = 0\n",
    "overal_correct_digits = 0\n",
    "overal_numbers = 0\n",
    "overal_correct_numbers = 0\n",
    "with open('test_results_no_preprocess.csv', 'w', newline='') as csv_file:\n",
    "\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    rep = 0\n",
    "    dummy_input = torch.randn(1, 3,224,224, dtype=torch.float).to(device)\n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = recognition_model(dummy_input)\n",
    "    timings = []\n",
    "\n",
    "    \n",
    "    writer = csv.writer(csv_file)\n",
    "    field = [\"filename\", \"backnumber\", \"amount_of_numbers\",\"digits_predicted\", \"number_predicted_correct\", \"number_of_predicted_digits\", \"inference_time\"]\n",
    "    writer.writerow(field)\n",
    "    progress = 0\n",
    "    for file in os.listdir(os.path.join('datasets','test', 'images')):\n",
    "\n",
    "        \n",
    "        img = cv2.imread(os.path.join('datasets','test','images', file))\n",
    "        starter.record()\n",
    "        results = recognition_model(img, verbose=False)\n",
    "        ender.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings.append(curr_time)\n",
    "        predicted_digits = []\n",
    "        average_x = []\n",
    "        result = results[0].boxes\n",
    "        digits_to_discard = []\n",
    "        boxes = []\n",
    "        for i in range(len(result.cls)):\n",
    "            for j in range(i+1, len(result.cls)):\n",
    "                box1 = (result.xyxy[i][0].item(), result.xyxy[i][1].item(), result.xyxy[i][2].item(), result.xyxy[i][3].item())\n",
    "                box2 = (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item())\n",
    "                iou = calculate_iou(box1, box2)\n",
    "                if iou > 0.5:\n",
    "                    if result.conf[i].item() > result.conf[j].item():\n",
    "                        digits_to_discard.append(j)\n",
    "                    else:\n",
    "                        digits_to_discard.append(i)\n",
    "                    \n",
    "        for j in range(len(result.cls)):\n",
    "            if j not in digits_to_discard:\n",
    "                predicted_digits.append(int(result.cls[j].item()))\n",
    "                boxes.append( (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item()))\n",
    "                average_x.append((result.xyxy[j][2].item()+result.xyxy[j][0].item())/2)\n",
    "            \n",
    "        predicted_digits = [x for _,x in sorted(zip(average_x,predicted_digits))]\n",
    "        predicted_boxes = [x for _,x in sorted(zip(average_x,boxes))]\n",
    "        predicted_number =\"\".join([str(i) for i in predicted_digits])\n",
    "        \n",
    "        filename = file.split('.')[0] +'.' + file.split('.')[1] +'.txt'\n",
    "        existing_filepath = os.path.join('datasets','test','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            # Using readlines()\n",
    "            file1 = open(existing_filepath, 'r')\n",
    "            Lines = file1.readlines()\n",
    "            boxes = []\n",
    "            count = 0\n",
    "            number = ''\n",
    "            digits = []\n",
    "            average_x = []\n",
    "            height, width, _ = img.shape\n",
    "            # Strips the newline character\n",
    "            for line in Lines:\n",
    "                count += 1\n",
    "                digits.append(line.split(' ')[0])\n",
    "                average_x.append(line.split(' ')[1])\n",
    "                box = ((float(line.split(' ')[1]) - float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) - float(line.split(' ')[4])/2)*height, (float(line.split(' ')[1]) + float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) + float(line.split(' ')[4])/2)*height)\n",
    "                boxes.append(box)\n",
    "            digits = [x for _, x in sorted(zip(average_x, digits))]\n",
    "            boxes = [x for _, x in sorted(zip(average_x, boxes))]\n",
    "            number =\"\".join([str(i) for i in digits])\n",
    "            correctly_predicted_digits = 0\n",
    "            already_found = []\n",
    "            for i in range(len(predicted_digits)):\n",
    "                for j in range(len(digits)):\n",
    "                    if j not in already_found:\n",
    "                        if calculate_iou(predicted_boxes[i], boxes[j]) >0.5:\n",
    "                            if predicted_digits[i] == int(digits[j]):\n",
    "                                correctly_predicted_digits += 1\n",
    "                                already_found.append(j)\n",
    "\n",
    "            overal_digits+= count\n",
    "            overal_numbers+= 1\n",
    "            overal_predicted_digits += len(predicted_digits)\n",
    "            overal_correct_digits += correctly_predicted_digits\n",
    "            if number == predicted_number:\n",
    "                overal_correct_numbers += 1\n",
    "            field = [filename, number, str(count), correctly_predicted_digits,number == predicted_number, predicted_digits, curr_time]\n",
    "            writer.writerow(field)\n",
    "            progress+=1\n",
    "        if progress % 500 == 0:\n",
    "            print(progress)\n",
    "        \"\"\"if progress >1:\n",
    "            break\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "71ad1bb7-2858-49ef-945d-5d576df56053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of digits in the testing dataset: 13920\n",
      "the number of digits that where predicted: 11915\n",
      "the digits that were predicted correctly: 10082\n",
      "the amount of numbers that were evaluated: 6000\n",
      "the numbers that werepredicted correct entirely: 3284\n",
      "average inference time: 19.62333707300822\n"
     ]
    }
   ],
   "source": [
    "print(f'the total number of digits in the testing dataset: {overal_digits}')\n",
    "print(f'the number of digits that where predicted: {overal_predicted_digits}')\n",
    "print(f'the digits that were predicted correctly: {overal_correct_digits}')\n",
    "print(f'the amount of numbers that were evaluated: {overal_numbers}')\n",
    "print(f'the numbers that werepredicted correct entirely: {overal_correct_numbers}')\n",
    "print(f'average inference time: {np.sum(np.array(timings))/len(timings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4869cfc-adbe-442a-88f7-0ffe54768f02",
   "metadata": {},
   "source": [
    "## testing with preprocessing, watershed based, perspective transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "adfa95f9-698d-46c4-adda-59e37765739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "78364b87-460b-4f60-ae9e-676e88983bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours_thresh(img):\n",
    "    gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv.threshold(gray,217,255,cv.THRESH_BINARY)\n",
    "    \n",
    "    \n",
    "    # noise removal\n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    opening = cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations = 1)\n",
    "    # sure background area\n",
    "    sure_bg = cv.dilate(opening,kernel,iterations=3)\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv.distanceTransform(opening,cv.DIST_L2,5)\n",
    "    ret, sure_fg = cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    "    # Finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv.subtract(sure_bg,sure_fg)\n",
    "    canvas = np.zeros((img.shape[0],img.shape[1],3), np.uint8)\n",
    "    contours, hierarchy = cv.findContours(unknown, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)\n",
    "    if len(contours) != 0:\n",
    "        cnt = sorted(contours, key=cv.contourArea, reverse=True)[0]\n",
    "        cv.drawContours(canvas, cnt, -1, (0, 255, 255))\n",
    "        return cnt\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5cc2edc2-6f2b-41eb-bb97-5b5c6d0e9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corners(cnt):\n",
    "    \n",
    "    def find_closest(edge):\n",
    "        min_dist = np.inf\n",
    "        closest = [0,0]\n",
    "        for point in cnt:\n",
    "            dist = (edge[0]-point[0][0])**2+(edge[1]-point[0][1])**2\n",
    "            if dist<min_dist:\n",
    "                min_dist = dist\n",
    "                closest = point[0]\n",
    "        return closest\n",
    "            \n",
    "    def order_corner_points(corners):\n",
    "        # Separate corners into individual points\n",
    "        # Index 0 - top-right\n",
    "        #       1 - top-left\n",
    "        #       2 - bottom-left\n",
    "        #       3 - bottom-right\n",
    "        nr = len(corners)\n",
    "        corners = np.array([[corner[0][0], corner[0][1]] for corner in corners])\n",
    "        average_x = corners[:,0].sum()/nr\n",
    "        average_y =  corners[:,1].sum()/nr\n",
    "        top_l, top_r, bottom_r, bottom_l = [None, None], [None, None],[None, None],[None, None]\n",
    "        for corner in corners:\n",
    "            if corner[1]<average_y:\n",
    "                if corner[0] < average_x:\n",
    "                    top_l = corner\n",
    "                else:\n",
    "                    top_r = corner\n",
    "            else:\n",
    "                if corner[0] < average_x:\n",
    "                    bottom_l = corner\n",
    "                else:\n",
    "                    bottom_r = corner\n",
    "        if top_l[0] == None:\n",
    "            #find point closest to [0,0]\n",
    "            top_l = find_closest([0,0])\n",
    "        if top_r[0] == None:\n",
    "            #find point closest to [cnt.shape[1]-1,0]\n",
    "            top_r = find_closest([original.shape[1]-1,0])\n",
    "        if bottom_l[0] == None:\n",
    "            #find point closest to [0,cnt.shape[0]-1]\n",
    "            bottom_l= find_closest([0,original.shape[0]-1])\n",
    "        if bottom_r[0] == None:\n",
    "            #find point closest to [cnt.shape[1]-1,cnt.shape[0]-1]\n",
    "            bottom_r= find_closest([original.shape[1]-1,original.shape[0]-1])\n",
    "        top_l[1] = top_l[1]-10 if top_l[1]-10>0 else 0\n",
    "        top_r[1] = top_r[1]-10 if top_r[1]-10>0 else 0\n",
    "        top_l[0] = top_l[0]-10 if top_l[0]-10>0 else 0\n",
    "        top_r[0] = top_r[0]+10 if top_r[0]+10 < original.shape[1] else original.shape[1]-1\n",
    "        return (top_l, top_r, bottom_r, bottom_l)\n",
    "    \n",
    "    \n",
    "    epsilon = 0.06 * cv.arcLength(cnt, True)\n",
    "    approx_corners = cv.approxPolyDP(cnt, epsilon, True)\n",
    "    corners = order_corner_points(approx_corners)\n",
    "    canvas = np.zeros((original.shape[0],original.shape[1],3), np.uint8)\n",
    "    cv.drawContours(canvas, cnt, -1, (0, 255, 255))\n",
    "\n",
    "\n",
    "    \n",
    "    return corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e4db8a80-a0d0-45ec-9857-52c0e1784977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transform(image, corners):\n",
    "\n",
    "    \n",
    "    top_l, top_r, bottom_r, bottom_l = corners\n",
    "    \n",
    "    ordered_corners = (top_l, top_r, bottom_r, bottom_l)\n",
    "\n",
    "    # Determine width of new image which is the max distance between \n",
    "    # (bottom right and bottom left) or (top right and top left) x-coordinates\n",
    "    width_A = np.sqrt(((bottom_r[0] - bottom_l[0]) ** 2) + ((bottom_r[1] - bottom_l[1]) ** 2))\n",
    "    width_B = np.sqrt(((top_r[0] - top_l[0]) ** 2) + ((top_r[1] - top_l[1]) ** 2))\n",
    "    width = max(int(width_A), int(width_B))\n",
    "\n",
    "    # Determine height of new image which is the max distance between \n",
    "    # (top right and bottom right) or (top left and bottom left) y-coordinates\n",
    "    height_A = np.sqrt(((top_r[0] - bottom_r[0]) ** 2) + ((top_r[1] - bottom_r[1]) ** 2))\n",
    "    height_B = np.sqrt(((top_l[0] - bottom_l[0]) ** 2) + ((top_l[1] - bottom_l[1]) ** 2))\n",
    "    height = max(int(height_A), int(height_B))\n",
    "\n",
    "    # Construct new points to obtain top-down view of image in \n",
    "    # top_r, top_l, bottom_l, bottom_r order\n",
    "    dimensions = np.array([[0, 0], [width - 1,0], [width - 1 , height - 1], \n",
    "                    [0,height - 1]], dtype = \"float32\")\n",
    "\n",
    "    # Convert to Numpy format\n",
    "    ordered_corners = np.array(ordered_corners, dtype=\"float32\")\n",
    "\n",
    "    # Find perspective transform matrix\n",
    "    matrix = cv.getPerspectiveTransform(ordered_corners, dimensions)\n",
    "    # Transform the image\n",
    "    transformed = cv.warpPerspective(image, matrix, (width, height),flags=cv.INTER_NEAREST,borderMode=cv.BORDER_REPLICATE)\n",
    "    \n",
    "    \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "794c6e89-80c3-4a42-a292-2c491c4e1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.536978244781494. Dividing input by 255.\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "overal_digits = 0\n",
    "overal_predicted_digits = 0\n",
    "overal_correct_digits = 0\n",
    "overal_numbers = 0\n",
    "overal_correct_numbers = 0\n",
    "with open('test_results_watershed_perspective.csv', 'w', newline='') as csv_file:\n",
    "\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    rep = 0\n",
    "    dummy_input = torch.randn(1, 3,224,224, dtype=torch.float).to(device)\n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = recognition_model(dummy_input)\n",
    "    timings = []\n",
    "\n",
    "    \n",
    "    writer = csv.writer(csv_file)\n",
    "    field = [\"filename\", \"backnumber\", \"amount_of_numbers\",\"digits_predicted\", \"number_predicted_correct\", \"number_of_predicted_digits\", \"inference_time\"]\n",
    "    writer.writerow(field)\n",
    "    progress = 0\n",
    "    for file in os.listdir(os.path.join('datasets','test', 'images')):\n",
    "\n",
    "        \n",
    "        img = cv2.imread(os.path.join('datasets','test','images', file))\n",
    "        original = np.copy(img)\n",
    "        img_copy = np.copy(img)\n",
    "        starter.record()\n",
    "        cnt = get_contours_thresh(img_copy)\n",
    "        if len(cnt)!= 0:\n",
    "            corners = get_corners(cnt)\n",
    "            if len(corners) <= 4:\n",
    "                img = perspective_transform(original, corners)\n",
    "        results = recognition_model(img, verbose=False)\n",
    "        ender.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings.append(curr_time)\n",
    "        predicted_digits = []\n",
    "        average_x = []\n",
    "        result = results[0].boxes\n",
    "        digits_to_discard = []\n",
    "        boxes = []\n",
    "        for i in range(len(result.cls)):\n",
    "            for j in range(i+1, len(result.cls)):\n",
    "                box1 = (result.xyxy[i][0].item(), result.xyxy[i][1].item(), result.xyxy[i][2].item(), result.xyxy[i][3].item())\n",
    "                box2 = (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item())\n",
    "                iou = calculate_iou(box1, box2)\n",
    "                if iou > 0.5:\n",
    "                    if result.conf[i].item() > result.conf[j].item():\n",
    "                        digits_to_discard.append(j)\n",
    "                    else:\n",
    "                        digits_to_discard.append(i)\n",
    "                    \n",
    "        for j in range(len(result.cls)):\n",
    "            if j not in digits_to_discard:\n",
    "                predicted_digits.append(int(result.cls[j].item()))\n",
    "                boxes.append( (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item()))\n",
    "                average_x.append((result.xyxy[j][2].item()+result.xyxy[j][0].item())/2)\n",
    "            \n",
    "        predicted_digits = [x for _,x in sorted(zip(average_x,predicted_digits))]\n",
    "        predicted_boxes = [x for _,x in sorted(zip(average_x,boxes))]\n",
    "        predicted_number =\"\".join([str(i) for i in predicted_digits])\n",
    "        \n",
    "        filename = file.split('.')[0] +'.' + file.split('.')[1] +'.txt'\n",
    "        existing_filepath = os.path.join('datasets','test','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            # Using readlines()\n",
    "            file1 = open(existing_filepath, 'r')\n",
    "            Lines = file1.readlines()\n",
    "            boxes = []\n",
    "            count = 0\n",
    "            number = ''\n",
    "            digits = []\n",
    "            average_x = []\n",
    "            height, width, _ = img.shape\n",
    "            # Strips the newline character\n",
    "            for line in Lines:\n",
    "                count += 1\n",
    "                digits.append(line.split(' ')[0])\n",
    "                average_x.append(line.split(' ')[1])\n",
    "                box = ((float(line.split(' ')[1]) - float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) - float(line.split(' ')[4])/2)*height, (float(line.split(' ')[1]) + float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) + float(line.split(' ')[4])/2)*height)\n",
    "                boxes.append(box)\n",
    "            digits = [x for _, x in sorted(zip(average_x, digits))]\n",
    "            boxes = [x for _, x in sorted(zip(average_x, boxes))]\n",
    "            number =\"\".join([str(i) for i in digits])\n",
    "            correctly_predicted_digits = 0\n",
    "            \"\"\"already_found = []\n",
    "            for i in range(len(predicted_digits)):\n",
    "                for j in range(len(digits)):\n",
    "                    if j not in already_found:\n",
    "                        if calculate_iou(predicted_boxes[i], boxes[j]) >0.5:\n",
    "                            if predicted_digits[i] == int(digits[j]):\n",
    "                                correctly_predicted_digits += 1\n",
    "                                already_found.append(j)\"\"\"\n",
    "\n",
    "            overal_digits+= count\n",
    "            overal_numbers+= 1\n",
    "            overal_predicted_digits += len(predicted_digits)\n",
    "            overal_correct_digits += correctly_predicted_digits\n",
    "            if number == predicted_number:\n",
    "                overal_correct_numbers += 1\n",
    "            field = [filename, number, str(count), correctly_predicted_digits,number == predicted_number, predicted_digits, curr_time]\n",
    "            writer.writerow(field)\n",
    "            progress+=1\n",
    "        if progress % 500 == 0:\n",
    "            print(progress)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "81442feb-05e0-45bc-ad8c-a15fc031f208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of digits in the testing dataset: 13920\n",
      "the number of digits that where predicted: 6240\n",
      "the digits that were predicted correctly: 0\n",
      "the amount of numbers that were evaluated: 6000\n",
      "the numbers that werepredicted correct entirely: 1550\n",
      "average inference time: 22.303172561327617\n"
     ]
    }
   ],
   "source": [
    "print(f'the total number of digits in the testing dataset: {overal_digits}')\n",
    "print(f'the number of digits that where predicted: {overal_predicted_digits}')\n",
    "print(f'the digits that were predicted correctly: {overal_correct_digits}')\n",
    "print(f'the amount of numbers that were evaluated: {overal_numbers}')\n",
    "print(f'the numbers that werepredicted correct entirely: {overal_correct_numbers}')\n",
    "print(f'average inference time: {np.sum(np.array(timings))/len(timings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45257f3-348d-4b97-a5de-4325751e740a",
   "metadata": {},
   "source": [
    "## now use homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e0cac6e4-7281-4ec2-9fa4-c1bbd4f609ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwarp(img, src):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: np.array\n",
    "        src: list\n",
    "        dst: list\n",
    "    Returns:\n",
    "        un_warped: np.array\n",
    "    \"\"\"\n",
    "    \n",
    "    top_l, top_r, bottom_r, bottom_l = src\n",
    "    \n",
    "    ordered_corners = (top_l, top_r, bottom_r, bottom_l)\n",
    "\n",
    "    # Determine width of new image which is the max distance between \n",
    "    # (bottom right and bottom left) or (top right and top left) x-coordinates\n",
    "    width_A = np.sqrt(((bottom_r[0] - bottom_l[0]) ** 2) + ((bottom_r[1] - bottom_l[1]) ** 2))\n",
    "    width_B = np.sqrt(((top_r[0] - top_l[0]) ** 2) + ((top_r[1] - top_l[1]) ** 2))\n",
    "    width = max(int(width_A), int(width_B))\n",
    "\n",
    "    # Determine height of new image which is the max distance between \n",
    "    # (top right and bottom right) or (top left and bottom left) y-coordinates\n",
    "    height_A = np.sqrt(((top_r[0] - bottom_r[0]) ** 2) + ((top_r[1] - bottom_r[1]) ** 2))\n",
    "    height_B = np.sqrt(((top_l[0] - bottom_l[0]) ** 2) + ((top_l[1] - bottom_l[1]) ** 2))\n",
    "    height = max(int(height_A), int(height_B))\n",
    "\n",
    "    # Construct new points to obtain top-down view of image in \n",
    "    # top_r, top_l, bottom_l, bottom_r order\n",
    "    dimensions = np.array([[0, 0], [width - 1,0],[0,height - 1], [width - 1 , height - 1]\n",
    "                    ], dtype = \"float32\")\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    H, _ = cv.findHomography(np.array([top_l,  top_r, bottom_l, bottom_r]), dimensions, method=cv.RANSAC, ransacReprojThreshold=3.0)\n",
    "    \n",
    "    un_warped = cv.warpPerspective(img, H, (w, h), flags=cv.INTER_LINEAR)\n",
    "\n",
    "    # plot\n",
    "    return un_warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "27df54f7-a52c-4b4d-9eb0-9b3c5ae71ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.292972564697266. Dividing input by 255.\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "overal_digits = 0\n",
    "overal_predicted_digits = 0\n",
    "overal_correct_digits = 0\n",
    "overal_numbers = 0\n",
    "overal_correct_numbers = 0\n",
    "with open('test_results_watershed_homography.csv', 'w', newline='') as csv_file:\n",
    "\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    rep = 0\n",
    "    dummy_input = torch.randn(1, 3,224,224, dtype=torch.float).to(device)\n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = recognition_model(dummy_input)\n",
    "    timings = []\n",
    "\n",
    "    \n",
    "    writer = csv.writer(csv_file)\n",
    "    field = [\"filename\", \"backnumber\", \"amount_of_numbers\",\"digits_predicted\", \"number_predicted_correct\", \"number_of_predicted_digits\", \"inference_time\"]\n",
    "    writer.writerow(field)\n",
    "    progress = 0\n",
    "    for file in os.listdir(os.path.join('datasets','test', 'images')):\n",
    "\n",
    "        \n",
    "        img = cv2.imread(os.path.join('datasets','test','images', file))\n",
    "        original = np.copy(img)\n",
    "        img_copy = np.copy(img)\n",
    "        starter.record()\n",
    "        cnt = get_contours_thresh(img_copy)\n",
    "        if len(cnt)!= 0:\n",
    "            corners = get_corners(cnt)\n",
    "            if len(corners) <= 4:\n",
    "                img = unwarp(original, corners)\n",
    "        results = recognition_model(img, verbose=False)\n",
    "        ender.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings.append(curr_time)\n",
    "        predicted_digits = []\n",
    "        average_x = []\n",
    "        result = results[0].boxes\n",
    "        digits_to_discard = []\n",
    "        boxes = []\n",
    "        for i in range(len(result.cls)):\n",
    "            for j in range(i+1, len(result.cls)):\n",
    "                box1 = (result.xyxy[i][0].item(), result.xyxy[i][1].item(), result.xyxy[i][2].item(), result.xyxy[i][3].item())\n",
    "                box2 = (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item())\n",
    "                iou = calculate_iou(box1, box2)\n",
    "                if iou > 0.5:\n",
    "                    if result.conf[i].item() > result.conf[j].item():\n",
    "                        digits_to_discard.append(j)\n",
    "                    else:\n",
    "                        digits_to_discard.append(i)\n",
    "                    \n",
    "        for j in range(len(result.cls)):\n",
    "            if j not in digits_to_discard:\n",
    "                predicted_digits.append(int(result.cls[j].item()))\n",
    "                boxes.append( (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item()))\n",
    "                average_x.append((result.xyxy[j][2].item()+result.xyxy[j][0].item())/2)\n",
    "            \n",
    "        predicted_digits = [x for _,x in sorted(zip(average_x,predicted_digits))]\n",
    "        predicted_boxes = [x for _,x in sorted(zip(average_x,boxes))]\n",
    "        predicted_number =\"\".join([str(i) for i in predicted_digits])\n",
    "\n",
    "        \n",
    "        filename = file.split('.')[0] +'.' + file.split('.')[1] +'.txt'\n",
    "        existing_filepath = os.path.join('datasets','test','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            # Using readlines()\n",
    "            file1 = open(existing_filepath, 'r')\n",
    "            Lines = file1.readlines()\n",
    "            boxes = []\n",
    "            count = 0\n",
    "            number = ''\n",
    "            digits = []\n",
    "            average_x = []\n",
    "            height, width, _ = img.shape\n",
    "            # Strips the newline character\n",
    "            for line in Lines:\n",
    "                count += 1\n",
    "                digits.append(line.split(' ')[0])\n",
    "                average_x.append(line.split(' ')[1])\n",
    "                box = ((float(line.split(' ')[1]) - float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) - float(line.split(' ')[4])/2)*height, (float(line.split(' ')[1]) + float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) + float(line.split(' ')[4])/2)*height)\n",
    "                boxes.append(box)\n",
    "            digits = [x for _, x in sorted(zip(average_x, digits))]\n",
    "            boxes = [x for _, x in sorted(zip(average_x, boxes))]\n",
    "            number =\"\".join([str(i) for i in digits])\n",
    "            correctly_predicted_digits = 0\n",
    "            already_found = []\n",
    "            \"\"\"for i in range(len(predicted_digits)):\n",
    "                for j in range(len(digits)):\n",
    "                    if j not in already_found:\n",
    "                        if calculate_iou(predicted_boxes[i], boxes[j]) >0.5:\n",
    "                            if predicted_digits[i] == int(digits[j]):\n",
    "                                correctly_predicted_digits += 1\n",
    "                                already_found.append(j)\"\"\"\n",
    "\n",
    "            overal_digits+= count\n",
    "            overal_numbers+= 1\n",
    "            overal_predicted_digits += len(predicted_digits)\n",
    "            overal_correct_digits += correctly_predicted_digits\n",
    "            if number == predicted_number:\n",
    "                overal_correct_numbers += 1\n",
    "            field = [filename, number, str(count), correctly_predicted_digits,number == predicted_number, predicted_digits, curr_time]\n",
    "            writer.writerow(field)\n",
    "            progress+=1\n",
    "\n",
    "        if progress % 500 == 0:\n",
    "            print(progress)\n",
    "        \"\"\"if progress >1:\n",
    "            break\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9aa32016-c0f4-47fe-9dee-bd0d175195d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of digits in the testing dataset: 13920\n",
      "the number of digits that where predicted: 7170\n",
      "the digits that were predicted correctly: 0\n",
      "the amount of numbers that were evaluated: 6000\n",
      "the numbers that werepredicted correct entirely: 1759\n",
      "average inference time: 22.622637280464172\n"
     ]
    }
   ],
   "source": [
    "print(f'the total number of digits in the testing dataset: {overal_digits}')\n",
    "print(f'the number of digits that where predicted: {overal_predicted_digits}')\n",
    "print(f'the digits that were predicted correctly: {overal_correct_digits}')\n",
    "print(f'the amount of numbers that were evaluated: {overal_numbers}')\n",
    "print(f'the numbers that werepredicted correct entirely: {overal_correct_numbers}')\n",
    "print(f'average inference time: {np.sum(np.array(timings))/len(timings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb13d0-1522-498b-8096-e6300ccad475",
   "metadata": {},
   "source": [
    "## now use kmeans first with perspective\n",
    "try later to use https://www.blog.dailydoseofds.com/p/make-sklearn-kmeans-20x-times-faster \\\n",
    "or try kmeans gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fd41ef0d-32d0-4bbd-93bf-3f2f727be42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# can initialize kmeans one time so don't add to time\n",
    "kmeans=KMeans(n_clusters=8, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f93f12c9-9366-4b97-8c84-c060d10d5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours_kmeans(img):\n",
    "    img=img.reshape((img.shape[1]*img.shape[0],3))\n",
    "    s=kmeans.fit(img)\n",
    "    labels=kmeans.labels_\n",
    "    centroid=kmeans.cluster_centers_\n",
    "    \n",
    "    good_labels = []\n",
    "    for i in range(len(centroid)):\n",
    "        if centroid[i].max() - centroid[i].min() <35 and centroid[i].min()>155:\n",
    "            good_labels.append(i)\n",
    "    mask = np.zeros(len(labels), dtype = np.uint8)\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in good_labels:\n",
    "            mask[i] = 255\n",
    "    mask = mask.reshape((original.shape[0],original.shape[1]))\n",
    "    contours, hierarchy = cv.findContours(mask, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)\n",
    "    if len(contours) != 0:\n",
    "        cnt = sorted(contours, key=cv.contourArea, reverse=True)[0]\n",
    "        return cnt\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a1909e45-3261-488c-ae66-7ab12698b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.81268835067749. Dividing input by 255.\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "overal_digits = 0\n",
    "overal_predicted_digits = 0\n",
    "overal_correct_digits = 0\n",
    "overal_numbers = 0\n",
    "overal_correct_numbers = 0\n",
    "with open('test_results_kmeans_perspective.csv', 'w', newline='') as csv_file:\n",
    "\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    rep = 0\n",
    "    dummy_input = torch.randn(1, 3,224,224, dtype=torch.float).to(device)\n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = recognition_model(dummy_input)\n",
    "    timings = []\n",
    "\n",
    "    \n",
    "    writer = csv.writer(csv_file)\n",
    "    field = [\"filename\", \"backnumber\", \"amount_of_numbers\",\"digits_predicted\", \"number_predicted_correct\", \"number_of_predicted_digits\", \"inference_time\"]\n",
    "    writer.writerow(field)\n",
    "    progress = 0\n",
    "    for file in os.listdir(os.path.join('datasets','test', 'images')):\n",
    "\n",
    "        \n",
    "        img = cv2.imread(os.path.join('datasets','test','images', file))\n",
    "        original = np.copy(img)\n",
    "        img_copy = np.copy(img)\n",
    "        starter.record()\n",
    "        #start_time = time.time()\n",
    "        cnt = get_contours_kmeans(img_copy)\n",
    "        if len(cnt)!= 0:\n",
    "            corners = get_corners(cnt)\n",
    "            if len(corners) <= 4:\n",
    "                img = perspective_transform(original, corners)\n",
    "        \n",
    "        #print(\"My program took\", time.time() - start_time, \"to run\")\n",
    "        results = recognition_model(img, verbose=False)\n",
    "        ender.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings.append(curr_time)\n",
    "        predicted_digits = []\n",
    "        average_x = []\n",
    "        result = results[0].boxes\n",
    "        digits_to_discard = []\n",
    "        boxes = []\n",
    "        for i in range(len(result.cls)):\n",
    "            for j in range(i+1, len(result.cls)):\n",
    "                box1 = (result.xyxy[i][0].item(), result.xyxy[i][1].item(), result.xyxy[i][2].item(), result.xyxy[i][3].item())\n",
    "                box2 = (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item())\n",
    "                iou = calculate_iou(box1, box2)\n",
    "                if iou > 0.5:\n",
    "                    if result.conf[i].item() > result.conf[j].item():\n",
    "                        digits_to_discard.append(j)\n",
    "                    else:\n",
    "                        digits_to_discard.append(i)\n",
    "                    \n",
    "        for j in range(len(result.cls)):\n",
    "            if j not in digits_to_discard:\n",
    "                predicted_digits.append(int(result.cls[j].item()))\n",
    "                boxes.append( (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item()))\n",
    "                average_x.append((result.xyxy[j][2].item()+result.xyxy[j][0].item())/2)\n",
    "            \n",
    "        predicted_digits = [x for _,x in sorted(zip(average_x,predicted_digits))]\n",
    "        predicted_boxes = [x for _,x in sorted(zip(average_x,boxes))]\n",
    "        predicted_number =\"\".join([str(i) for i in predicted_digits])\n",
    "        \n",
    "        filename = file.split('.')[0] +'.' + file.split('.')[1] +'.txt'\n",
    "        existing_filepath = os.path.join('datasets','test','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            # Using readlines()\n",
    "            file1 = open(existing_filepath, 'r')\n",
    "            Lines = file1.readlines()\n",
    "            boxes = []\n",
    "            count = 0\n",
    "            number = ''\n",
    "            digits = []\n",
    "            average_x = []\n",
    "            height, width, _ = img.shape\n",
    "            # Strips the newline character\n",
    "            for line in Lines:\n",
    "                count += 1\n",
    "                digits.append(line.split(' ')[0])\n",
    "                average_x.append(line.split(' ')[1])\n",
    "                box = ((float(line.split(' ')[1]) - float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) - float(line.split(' ')[4])/2)*height, (float(line.split(' ')[1]) + float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) + float(line.split(' ')[4])/2)*height)\n",
    "                boxes.append(box)\n",
    "            digits = [x for _, x in sorted(zip(average_x, digits))]\n",
    "            boxes = [x for _, x in sorted(zip(average_x, boxes))]\n",
    "            number =\"\".join([str(i) for i in digits])\n",
    "            correctly_predicted_digits = 0\n",
    "            already_found = []\n",
    "            \"\"\"for i in range(len(predicted_digits)):\n",
    "                for j in range(len(digits)):\n",
    "                    if j not in already_found:\n",
    "                        if calculate_iou(predicted_boxes[i], boxes[j]) >0.5:\n",
    "                            if predicted_digits[i] == int(digits[j]):\n",
    "                                correctly_predicted_digits += 1\n",
    "                                already_found.append(j)\"\"\"\n",
    "\n",
    "            overal_digits+= count\n",
    "            overal_numbers+= 1\n",
    "            overal_predicted_digits += len(predicted_digits)\n",
    "            overal_correct_digits += correctly_predicted_digits\n",
    "            if number == predicted_number:\n",
    "                overal_correct_numbers += 1\n",
    "            field = [filename, number, str(count), correctly_predicted_digits,number == predicted_number, predicted_digits, curr_time]\n",
    "            writer.writerow(field)\n",
    "            progress+=1\n",
    "        if progress % 500 == 0:\n",
    "            print(progress)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ea1533cb-0810-4a1f-bb19-84054d4720d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of digits in the testing dataset: 13920\n",
      "the number of digits that where predicted: 8792\n",
      "the digits that were predicted correctly: 0\n",
      "the amount of numbers that were evaluated: 6000\n",
      "the numbers that werepredicted correct entirely: 2224\n",
      "average inference time: 117.99039589977265\n"
     ]
    }
   ],
   "source": [
    "print(f'the total number of digits in the testing dataset: {overal_digits}')\n",
    "print(f'the number of digits that where predicted: {overal_predicted_digits}')\n",
    "print(f'the digits that were predicted correctly: {overal_correct_digits}')\n",
    "print(f'the amount of numbers that were evaluated: {overal_numbers}')\n",
    "print(f'the numbers that werepredicted correct entirely: {overal_correct_numbers}')\n",
    "print(f'average inference time: {np.sum(np.array(timings))/len(timings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57bbb9-de33-4c0d-b250-9d3c92768441",
   "metadata": {},
   "source": [
    "## now use homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "175458b0-82ea-447f-8cb2-36fe7be4e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.424120903015137. Dividing input by 255.\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "overal_digits = 0\n",
    "overal_predicted_digits = 0\n",
    "overal_correct_digits = 0\n",
    "overal_numbers = 0\n",
    "overal_correct_numbers = 0\n",
    "with open('test_results_kmeans_homography.csv', 'w', newline='') as csv_file:\n",
    "\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    rep = 0\n",
    "    dummy_input = torch.randn(1, 3,224,224, dtype=torch.float).to(device)\n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = recognition_model(dummy_input)\n",
    "    timings = []\n",
    "\n",
    "    \n",
    "    writer = csv.writer(csv_file)\n",
    "    field = [\"filename\", \"backnumber\", \"amount_of_numbers\",\"digits_predicted\", \"number_predicted_correct\", \"number_of_predicted_digits\", \"inference_time\"]\n",
    "    writer.writerow(field)\n",
    "    progress = 0\n",
    "    for file in os.listdir(os.path.join('datasets','test', 'images')):\n",
    "\n",
    "        \n",
    "        img = cv2.imread(os.path.join('datasets','test','images', file))\n",
    "        original = np.copy(img)\n",
    "        img_copy = np.copy(img)\n",
    "        starter.record()\n",
    "        #start_time = time.time()\n",
    "        cnt = get_contours_kmeans(img_copy)\n",
    "        if len(cnt)!= 0:\n",
    "            corners = get_corners(cnt)\n",
    "            if len(corners) <= 4:\n",
    "                img = unwarp(original, corners)\n",
    "        #print(\"My program took\", time.time() - start_time, \"to run\")\n",
    "        results = recognition_model(img, verbose=False)\n",
    "        ender.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings.append(curr_time)\n",
    "        predicted_digits = []\n",
    "        average_x = []\n",
    "        result = results[0].boxes\n",
    "        digits_to_discard = []\n",
    "        boxes = []\n",
    "        for i in range(len(result.cls)):\n",
    "            for j in range(i+1, len(result.cls)):\n",
    "                box1 = (result.xyxy[i][0].item(), result.xyxy[i][1].item(), result.xyxy[i][2].item(), result.xyxy[i][3].item())\n",
    "                box2 = (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item())\n",
    "                iou = calculate_iou(box1, box2)\n",
    "                if iou > 0.5:\n",
    "                    if result.conf[i].item() > result.conf[j].item():\n",
    "                        digits_to_discard.append(j)\n",
    "                    else:\n",
    "                        digits_to_discard.append(i)\n",
    "                    \n",
    "        for j in range(len(result.cls)):\n",
    "            if j not in digits_to_discard:\n",
    "                predicted_digits.append(int(result.cls[j].item()))\n",
    "                boxes.append( (result.xyxy[j][0].item(), result.xyxy[j][1].item(), result.xyxy[j][2].item(), result.xyxy[j][3].item()))\n",
    "                average_x.append((result.xyxy[j][2].item()+result.xyxy[j][0].item())/2)\n",
    "            \n",
    "        predicted_digits = [x for _,x in sorted(zip(average_x,predicted_digits))]\n",
    "        predicted_boxes = [x for _,x in sorted(zip(average_x,boxes))]\n",
    "        predicted_number =\"\".join([str(i) for i in predicted_digits])\n",
    "        \n",
    "        filename = file.split('.')[0] +'.' + file.split('.')[1] +'.txt'\n",
    "        existing_filepath = os.path.join('datasets','test','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            # Using readlines()\n",
    "            file1 = open(existing_filepath, 'r')\n",
    "            Lines = file1.readlines()\n",
    "            boxes = []\n",
    "            count = 0\n",
    "            number = ''\n",
    "            digits = []\n",
    "            average_x = []\n",
    "            height, width, _ = img.shape\n",
    "            # Strips the newline character\n",
    "            for line in Lines:\n",
    "                count += 1\n",
    "                digits.append(line.split(' ')[0])\n",
    "                average_x.append(line.split(' ')[1])\n",
    "                box = ((float(line.split(' ')[1]) - float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) - float(line.split(' ')[4])/2)*height, (float(line.split(' ')[1]) + float(line.split(' ')[3])/2)*width,(float(line.split(' ')[2]) + float(line.split(' ')[4])/2)*height)\n",
    "                boxes.append(box)\n",
    "            digits = [x for _, x in sorted(zip(average_x, digits))]\n",
    "            boxes = [x for _, x in sorted(zip(average_x, boxes))]\n",
    "            number =\"\".join([str(i) for i in digits])\n",
    "            correctly_predicted_digits = 0\n",
    "            already_found = []\n",
    "            \"\"\"for i in range(len(predicted_digits)):\n",
    "                for j in range(len(digits)):\n",
    "                    if j not in already_found:\n",
    "                        if calculate_iou(predicted_boxes[i], boxes[j]) >0.5:\n",
    "                            if predicted_digits[i] == int(digits[j]):\n",
    "                                correctly_predicted_digits += 1\n",
    "                                already_found.append(j)\"\"\"\n",
    "\n",
    "            overal_digits+= count\n",
    "            overal_numbers+= 1\n",
    "            overal_predicted_digits += len(predicted_digits)\n",
    "            overal_correct_digits += correctly_predicted_digits\n",
    "            if number == predicted_number:\n",
    "                overal_correct_numbers += 1\n",
    "            field = [filename, number, str(count), correctly_predicted_digits,number == predicted_number, predicted_digits, curr_time]\n",
    "            writer.writerow(field)\n",
    "            progress+=1\n",
    "        if progress % 500 == 0:\n",
    "            print(progress)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "571767e0-44f6-46b8-9b1e-dd82b66bb81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of digits in the testing dataset: 13920\n",
      "the number of digits that where predicted: 9347\n",
      "the digits that were predicted correctly: 0\n",
      "the amount of numbers that were evaluated: 6000\n",
      "the numbers that werepredicted correct entirely: 2278\n",
      "average inference time: 117.96601511955261\n"
     ]
    }
   ],
   "source": [
    "print(f'the total number of digits in the testing dataset: {overal_digits}')\n",
    "print(f'the number of digits that where predicted: {overal_predicted_digits}')\n",
    "print(f'the digits that were predicted correctly: {overal_correct_digits}')\n",
    "print(f'the amount of numbers that were evaluated: {overal_numbers}')\n",
    "print(f'the numbers that werepredicted correct entirely: {overal_correct_numbers}')\n",
    "print(f'average inference time: {np.sum(np.array(timings))/len(timings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0203a8-4a9f-458f-bf6b-a05d56c11cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
